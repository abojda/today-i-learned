{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TL;DR**\n",
    ">\n",
    ">- Default collate behavior in PyTorch DataLoader depends on the type of the object/collection returned from the PyTorch dataset\n",
    ">- By default, DataLoader uses the [default_collate](https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/_utils/collate.py#L204) function to collate lists of samples into batches\n",
    ">- To check how different data types are handled by `default_collate` we can investigate examples in the docstring of this function\n",
    ">- It is also possible to write custom `collate_fn` - examples in sections 1.3 and 2.3 below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Returning image Tensor and int target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TensorIntDataset(Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        self.imgs = [torch.rand(3, 2, 2) for i in range(n_samples)]\n",
    "        self.targets = np.random.randint(0, 9, size=n_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.targets[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "\n",
    "ti_dataset = TensorIntDataset(10)\n",
    "ti_dataloader = DataLoader(ti_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single sample from this dataset is an image tensor in CHW format and `int` target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "img, target = ti_dataset[0]\n",
    "print(img.shape)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can expect the Dataloader will return the batch of images in NCHW format and the tensor with targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2, 2])\n",
      "tensor([3, 5])\n"
     ]
    }
   ],
   "source": [
    "imgs, targets = next(iter(ti_dataloader))\n",
    "print(imgs.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Returning image Tensor and dictionary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDictDataset(Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        self.imgs = [torch.rand(3, 2, 2) for i in range(n_samples)]\n",
    "        self.targets = [\n",
    "            {\"label\": np.random.randint(0, 9), \"other_value\": np.random.randint(0, 9)}\n",
    "            for i in range(n_samples)\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.targets[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "\n",
    "td_dataset = TensorDictDataset(10)\n",
    "td_dataloader = DataLoader(td_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single sample from this dataset is an image tensor in CHW format and the target dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n",
      "{'label': 1, 'other_value': 2}\n"
     ]
    }
   ],
   "source": [
    "img, target = td_dataset[0]\n",
    "print(img.shape)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, based on example 1.1, we might expect that the Dataloader will return the target as a list of dictionaries - for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "    {\n",
    "        \"label\": 4,\n",
    "        \"other_value\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"label\": 2,\n",
    "        \"other_value\": 6,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not the case!\n",
    "\n",
    "In fact, the Dataloader will return the batch of images in NCHW format and the **single target dictionary containing targets for all the samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2, 2])\n",
      "{'label': tensor([1, 5]), 'other_value': tensor([2, 3])}\n"
     ]
    }
   ],
   "source": [
    "imgs, targets = next(iter(td_dataloader))\n",
    "print(imgs.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, DataLoader uses the [default_collate](https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/_utils/collate.py#L204) function to collate lists of samples into batches.\n",
    "\n",
    "To check how different data types are handled by `default_collate` we can investigate the docstring of this function - for example, behavior for `Mapping` is described [here](https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/_utils/collate.py#L238) and we can see that it matches the output format we obtained above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Modifying collate_fn\n",
    "To modify collate behavior for our specific needs we can write custom collate function based on the [hint from the docstring](https://github.com/pytorch/pytorch/blob/v2.0.1/torch/utils/data/_utils/collate.py#L251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    if isinstance(batch, list) and len(batch[0]) == 2 and isinstance(batch[0][1], dict):\n",
    "        imgs = torch.stack([img for img, target in batch])\n",
    "        targets = [target for img, target in batch]\n",
    "        return imgs, targets\n",
    "    else:  # Fall back to `default_collate`\n",
    "        return torch.utils.data.default_collate(batch)\n",
    "\n",
    "\n",
    "td_dataloader = DataLoader(td_dataset, batch_size=2, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2, 2])\n",
      "[{'label': 1, 'other_value': 2}, {'label': 5, 'other_value': 3}]\n"
     ]
    }
   ],
   "source": [
    "imgs, targets = next(iter(td_dataloader))\n",
    "print(imgs.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Real-life example\n",
    "# 2.1 Motivation\n",
    "Let's imagine the following situation. We work with a `fasterrcnn_resnet50_fpn` object detection model from the torchvision library.\n",
    "\n",
    "During training, the [model](https://pytorch.org/vision/0.15/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html) expects both the input tensors and a list of target dictionaries containing ground-truth boxes and labels with the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_boxes(n):\n",
    "    \"\"\"\n",
    "    Generate \"random\" bounding boxes ensuring x2>x1 and y2>y1\n",
    "    Only for presentation purposes\n",
    "    \"\"\"\n",
    "    xy1 = 0.9 * torch.rand(n, 2)\n",
    "    xy2 = xy1 + 0.1\n",
    "\n",
    "    return torch.cat([xy1, xy2], dim=1)\n",
    "\n",
    "\n",
    "imgs = [torch.rand(3, 2, 2), torch.rand(3, 2, 2)]  # 2 RGB images (2x2 size)\n",
    "targets = [\n",
    "    {\n",
    "        # Ground-truth for the first image\n",
    "        # 5 boxes with [x1, y1, x2, y2] coordinates and 5 COCO class labels\n",
    "        \"boxes\": rand_boxes(5),  # torch.Size([5, 4])\n",
    "        \"labels\": torch.randint(low=0, high=91, size=(5,)),\n",
    "    },\n",
    "    {\n",
    "        # Ground-truth for the second image\n",
    "        # 7 boxes with [x1, y1, x2, y2] coordinates and 7 COCO class labels\n",
    "        \"boxes\": rand_boxes(7),  # torch.Size([7, 4])\n",
    "        \"labels\": torch.randint(low=0, high=91, size=(7,)),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(0.3192, grad_fn=<NllLossBackward0>),\n",
       " 'loss_box_reg': tensor(0.0085, grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(2.0769, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_rpn_box_reg': tensor(0.0861, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn,\n",
    "    FasterRCNN_ResNet50_FPN_Weights,\n",
    ")\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "model.train()\n",
    "model(imgs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Initial implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the PyTorch dataset that will return the data in this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_target():\n",
    "    n_objects = np.random.randint(1, 10)\n",
    "    target = {\n",
    "        \"boxes\": rand_boxes(n_objects),\n",
    "        \"labels\": torch.randint(low=0, high=91, size=(n_objects,)),\n",
    "    }\n",
    "    return target\n",
    "\n",
    "\n",
    "class DetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        self.imgs = [torch.rand(3, 2, 2) for i in range(n_samples)]\n",
    "        self.targets = [rand_target() for i in range(n_samples)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.targets[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n",
      "{'boxes': tensor([[0.5160, 0.4331, 0.6160, 0.5331],\n",
      "        [0.0248, 0.1283, 0.1248, 0.2283],\n",
      "        [0.1320, 0.6417, 0.2320, 0.7417],\n",
      "        [0.5979, 0.1503, 0.6979, 0.2503],\n",
      "        [0.4593, 0.6163, 0.5593, 0.7163],\n",
      "        [0.2641, 0.8964, 0.3641, 0.9964],\n",
      "        [0.3178, 0.7418, 0.4178, 0.8418]]), 'labels': tensor([12, 56, 19, 37, 83, 23, 79])}\n"
     ]
    }
   ],
   "source": [
    "detection_dataset = DetectionDataset(10)\n",
    "\n",
    "img, target = detection_dataset[0]\n",
    "print(img.shape)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single sample returned from the dataset matches the format required by the `torchvision` model\n",
    "\n",
    "However, if we use the DataLoader with the default collate function, the format of the batched data will be incorrect or we might even encounter `RuntimeError` if the number of targets is different for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_dataloader = DataLoader(detection_dataset, batch_size=2)\n",
    "\n",
    "# The below code will throw RuntimeError or TypeError due to the data format problems\n",
    "\n",
    "# imgs, targets = next(iter(detection_dataloader))\n",
    "# model.train()\n",
    "# model(imgs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to use the custom collate function similar to `custom_collate` we introduced in section 1.3\n",
    "- We'll just return the list of CHW image tensors instead of a single NCHW tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_detection_collate(batch):\n",
    "    if isinstance(batch, list) and len(batch[0]) == 2 and isinstance(batch[0][1], dict):\n",
    "        imgs = [img for img, target in batch]\n",
    "        targets = [target for img, target in batch]\n",
    "        return imgs, targets\n",
    "    else:  # Fall back to `default_collate`\n",
    "        return torch.utils.data.default_collate(batch)\n",
    "\n",
    "\n",
    "detection_dataloader = DataLoader(\n",
    "    detection_dataset, batch_size=2, collate_fn=custom_detection_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 images of size: torch.Size([3, 2, 2])\n",
      "[{'boxes': tensor([[0.5160, 0.4331, 0.6160, 0.5331],\n",
      "        [0.0248, 0.1283, 0.1248, 0.2283],\n",
      "        [0.1320, 0.6417, 0.2320, 0.7417],\n",
      "        [0.5979, 0.1503, 0.6979, 0.2503],\n",
      "        [0.4593, 0.6163, 0.5593, 0.7163],\n",
      "        [0.2641, 0.8964, 0.3641, 0.9964],\n",
      "        [0.3178, 0.7418, 0.4178, 0.8418]]), 'labels': tensor([12, 56, 19, 37, 83, 23, 79])}, {'boxes': tensor([[0.2821, 0.0579, 0.3821, 0.1579],\n",
      "        [0.7718, 0.6386, 0.8718, 0.7386]]), 'labels': tensor([81,  7])}]\n"
     ]
    }
   ],
   "source": [
    "imgs, targets = next(iter(detection_dataloader))\n",
    "\n",
    "print(f\"{len(imgs)} images of size: {imgs[0].shape}\")\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(0.2132, grad_fn=<NllLossBackward0>),\n",
       " 'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(1.4646, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_rpn_box_reg': tensor(0.0683, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "model(imgs, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_book",
   "language": "python",
   "name": "ml_book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
